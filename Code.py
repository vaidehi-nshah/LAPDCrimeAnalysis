# -*- coding: utf-8 -*-
"""Term_Project_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dS1htVeqENafqtlrrU7J4J25o4StwHS7
"""

!pip install pyspark

from google.colab import drive
drive.mount('/content/gdrive',force_remount=True)

import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.types import DateType
from pyspark.sql.functions import to_date, col, regexp_replace, concat_ws, lit, coalesce
from pyspark.sql import functions as F

# Spark Session
spark = SparkSession.builder.appName("CS777_Project").getOrCreate()

# Reading data into Spark Dataframe
# VAIDEHI
file_path = "/content/gdrive/MyDrive/FALL 23/CS777/Term Project/Crime_Data_from_2020_to_Present.csv"

# ADITYA
# file_path = ""

# SARTHAK
# file_path = "/content/gdrive/MyDrive/CS777 Final Project/Data/Crime_Data_from_2020_to_Present.csv"

df_spark = spark.read.csv(file_path, header=True, inferSchema=True)
df_spark.show(3)

# Drop columns
columns_to_drop = ["DR_NO", "Weapon Used Cd", "Crm Cd 1", "Crm Cd 2", "Crm Cd 3", "Crm Cd 4"]
df_spark = df_spark.drop(*columns_to_drop)

df_spark.show(3)

# Dataframe Schema
df_spark.printSchema()

spark.conf.set("spark.sql.legacy.timeParserPolicy", "LEGACY")

df_spark = df_spark.withColumn("Date Rptd", regexp_replace(col("Date Rptd"), ' 12:00:00 AM', ''))
df_spark = df_spark.withColumn("DATE OCC", regexp_replace(col("DATE OCC"), ' 12:00:00 AM', ''))

df_spark.show(5)

# Date format set
df_spark = df_spark.withColumn("Date Rptd", to_date(df_spark["Date Rptd"], "MM/dd/yyyy"))
df_spark = df_spark.withColumn("Date Rptd", df_spark["Date Rptd"].cast(DateType()))

df_spark = df_spark.withColumn("DATE OCC", to_date(df_spark["DATE OCC"], "MM/dd/yyyy"))
df_spark = df_spark.withColumn("DATE OCC", df_spark["DATE OCC"].cast(DateType()))

# Dataframe Schema
df_spark.printSchema()

# merging two columns for address
df_spark = df_spark.withColumn("address", concat_ws(" ", coalesce(col("LOCATION"), lit("")), coalesce(col("Cross Street"), lit(""))))

df_spark = df_spark.withColumn("address", regexp_replace(col("address"), "\s+", " "))

columns_to_drop = ["LOCATION", "Cross Street"]
df_spark = df_spark.drop(*columns_to_drop)

df_spark.show(3)

# Renaming all the columns
new_column_names = ['DateRptd', 'DateOcc', 'TimeOcc', 'AreaCd', 'AreaName', 'RptDistNo', 'CrimeType', 'CrmCd', 'CrmCdDesc', 'Mocodes', 'VictAge', 'VictSex', 'VictDescent', 'PremisCd', 'PremisDesc', 'Weapon', 'Status', 'StatusDesc', 'Latitude', 'Longitude', 'Address']
df_spark = df_spark.toDF(*new_column_names)

df_spark.show(3)

# Define the mapping of values to be replaced
descent_mapping = {
    'A': 'Other Asian',
    'B': 'Black',
    'C': 'Chinese',
    'D': 'Cambodian',
    'F': 'Filipino',
    'G': 'Guamanian',
    'H': 'Hispanic/Latin/Mexican',
    'I': 'American Indian/Alaskan Native',
    'J': 'Japanese',
    'K': 'Korean',
    'L': 'Laotian',
    'O': 'Other',
    'P': 'Pacific Islander',
    'S': 'Samoan',
    'U': 'Hawaiian',
    'V': 'Vietnamese',
    'W': 'White',
    'X': 'None',
    'Z': 'Asian Indian',
    'None': 'Other',
    '-': 'Other'
}

# Replace values in the 'VictDescent' column using when() and otherwise()
df_spark = df_spark.withColumn('VictDescent',
                               F.when(F.col('VictDescent').isin(list(descent_mapping.keys())),
                                      F.create_map([F.lit(x) for x in sum(descent_mapping.items(), ())])[F.col('VictDescent')])
                                .otherwise(F.col('VictDescent')))

# Show the updated DataFrame
df_spark.show(10)

# Find unique values in 'VictSex' column
unique_vict_sex = df_spark.select('VictSex').distinct().rdd.flatMap(lambda x: x).collect()

# Display unique values
print(unique_vict_sex)

# Replace blank values in 'VictDesc' column with 'None'
df_spark = df_spark.withColumn('VictDescent', F.when(col('VictDescent').isNull() , 'NULL').otherwise(df_spark['VictDescent']))

# Get counts of unique values in 'VictDescent' column
counts_per_vict_descent = df_spark.groupBy('VictDescent').count().orderBy('count', ascending=False)

# Show unique values with counts
counts_per_vict_descent.show()

# Get counts of unique values in 'VictSex' column
counts_per_vict_sex = df_spark.groupBy('VictSex').count().orderBy('count', ascending=False)

# Show unique values with counts
counts_per_vict_sex.show()

# Replace blank values in 'VictDesc' column with 'None'
df_spark = df_spark.withColumn('VictSex', F.when(col('VictSex').isNull() , 'NULL').otherwise(df_spark['VictSex']))
df_spark = df_spark.withColumn('VictSex', F.when(col('VictSex')=='H' , 'X').otherwise(df_spark['VictSex']))
df_spark = df_spark.withColumn('VictSex', F.when(col('VictSex')=='-' , 'X').otherwise(df_spark['VictSex']))

# Get counts of unique values in 'VictSex' column
counts_per_vict_sex = df_spark.groupBy('VictSex').count().orderBy('count', ascending=False)

# Show unique values with counts
counts_per_vict_sex.show()

# Get column names
columns = df_spark.columns

# Check for NULL values in each column
for col_name in columns:
    null_count = df_spark.filter(col(col_name).isNull() | (col(col_name) == '')).count()
    print(f"Column '{col_name}': {null_count} NULL values")

# Get counts of unique values in 'Weapon' column
counts_per_weapon = df_spark.groupBy('Weapon').count().orderBy('count', ascending=False)

# Show unique values with counts
counts_per_weapon.show()

df_spark = df_spark.withColumn('Weapon', F.when(col('Weapon').isNull() , 'NO WEAPON INFO').otherwise(df_spark['Weapon']))

# Get counts of unique values in 'Weapon' column
counts_per_weapon = df_spark.groupBy('Weapon').count().orderBy('count', ascending=False)

# Show unique values with counts
counts_per_weapon.show()

# Drop rows with NULL values in 'PremisCd' or 'PremisDesc' columns
df_spark = df_spark.filter((col('PremisCd').isNotNull()) & (col('PremisDesc').isNotNull()))

# Show the cleaned DataFrame
df_spark.show()

# Get column names
columns = df_spark.columns

# Check for NULL values in each column
for col_name in columns:
    null_count = df_spark.filter(col(col_name).isNull() | (col(col_name) == '')).count()
    print(f"Column '{col_name}': {null_count} NULL values")

from pyspark.sql.functions import count, when

# Assuming 'df_spark' is your PySpark DataFrame

# Get the count of non-null values in each column
column_counts = df_spark.agg(*[
    count(when(col(c).isNotNull(), c)).alias(c + '_count') for c in df_spark.columns
])

# Show the count of non-null values in each column
column_counts.show()

df_spark.show(5)

# Dataframe Schema
df_spark.printSchema()

"""#Geospatial Analysis"""

geojson_file_path = '/content/gdrive/MyDrive/CS777 Final Project/Data/LAPD_Reporting_District.geojson'

geospatial_data = df_spark.select("CrimeType", "RptDistNo", "Latitude", "Longitude").toPandas()
geospatial_data.head(5)

import folium
from folium.plugins import MarkerCluster
from shapely.geometry import Point
from pyspark.sql import SparkSession
import geopandas as gpd

# Create a base map centered around LA
m = folium.Map(location=[34.0522, -118.2437], zoom_start=10, min_zoom=10, max_zoom=20)

# Create a Spark session
#spark = SparkSession.builder.appName("ChoroplethMap").getOrCreate()

# Assuming you already have the GeoJSON file path and the df_spark dataset
geojson_file_path = '/content/gdrive/MyDrive/CS777 Final Project/Data/LAPD_Reporting_District.geojson'

# Load the GeoJSON file using GeoPandas
gdf = gpd.read_file(geojson_file_path)

# Take a subset of 10k rows from df_spark
subset_df_spark = df_spark.limit(25000)

# Select relevant columns
geospatial_data = subset_df_spark.select("CrimeType", "RptDistNo", "Latitude", "Longitude").toPandas()

# Merge GeoDataFrame with the df_spark subset based on RptDistNo and REPDIST columns
merged_gdf = gdf.merge(geospatial_data, how='right', left_on='REPDIST', right_on='RptDistNo')

# Create a GeoDataFrame from the merged data
geometry = [Point(xy) for xy in zip(merged_gdf['Longitude'], merged_gdf['Latitude'])]
gdf_subset = gpd.GeoDataFrame(merged_gdf, geometry=geometry, crs=gdf.crs)

# Create a map
m = folium.Map(location=[34.05, -118.25], zoom_start=10)

marker_cluster = MarkerCluster().add_to(m)
for row in subset_df_spark.collect():
    folium.Marker(
        location=[row['Latitude'], row['Longitude']],
        popup=f"Crime Type: {row['CrimeType']}",
    ).add_to(marker_cluster)

# Save the map as an HTML file
#map_file_path = '/content/gdrive/MyDrive/FALL 23/CS777/Term Project/marker_cluster_map.html'
#m.save(map_file_path)

# Display the map
m

"""# EDA"""

from pyspark.sql.functions import split

# Split 'Mocodes' column by space (' ') delimiter
split_codes_df = df_spark.withColumn('SplitCodes', split(df_spark['Mocodes'], ' '))

# Show DataFrame with split codes
split_codes_df.show()

from pyspark.sql.functions import explode

# Explode the split codes to count individual occurrences
exploded_df = split_codes_df.select(explode('SplitCodes').alias('IndividualCode')).filter('IndividualCode != ""')

# Count occurrences of each individual code
individual_code_counts = exploded_df.groupBy('IndividualCode').count().orderBy('count', ascending=False)
individual_code_counts.show()

"""# Top 10 Mocodes

1822 - Stranger

0344 - Removes Vict Property

0913 - Victim knew suspect

0329 - Vandalized

0416 - Hit-Hit w/ weapon

1300 - Vehicle involved

2000 - Domestic violence

0400 - Force used

1402 - Evidence Booked (any crime)

2004 - Suspect is homeless/ transient
"""

import matplotlib.pyplot as plt

# Assuming you have the 'individual_code_counts' DataFrame containing individual code counts

# Get the top 50 occurring Mocodes by count
top_10_codes = individual_code_counts.limit(10)

# Convert PySpark DataFrame to Pandas for visualization
top_10_codes_pd = top_10_codes.toPandas()

descriptions = {
    '1822': 'Stranger',
    '0344': 'Removes\nVict Property',
    '0913': 'Victim knew suspect',
    '0329': 'Vandalized',
    '0416': 'Hit-Hit w/ weapon',
    '1300': 'Vehicle involved',
    '2000': 'Domestic violence',
    '0400': 'Force used',
    '1402': 'Evidence Booked\n(any crime)',
    '2004': 'Suspect is\nhomeless/ transient'
}

plt.figure(figsize=(15, 8))  # Adjust figure size to accommodate the labels
bars = plt.bar(top_10_codes_pd['IndividualCode'], top_10_codes_pd['count'])
plt.xlabel('Individual Code')
plt.ylabel('Frequency')
plt.title('Top 10 Most Frequently used Mocodes')

# Set custom tick labels with rotation and handling long labels
tick_labels = [f"{code}\n{descriptions[code]}" for code in top_10_codes_pd['IndividualCode']]  # Use \n for line break
plt.xticks(range(len(tick_labels)), labels=tick_labels, rotation=0, ha='center')

# Adjust layout to prevent overlapping
plt.tight_layout()
plt.show()

"""# BUSINESS PROBLEM 1:

Does the demographic of a person make them more susceptible to crime? And the type of crime?
"""

df_spark.show(5)



"""VictAge, VictSex, VictDescent, CrimeType"""

# Selecting specific columns and filtering ages above 0
df_prob1 = df_spark.select("VictAge", "VictSex", "VictDescent", "CrimeType").filter((col("VictAge") > 0) & (col("VictSex") != 'NULL') & (col("VictDescent") != 'None')& (col("VictDescent") != 'NULL'))

# Displaying the schema to ensure the columns are selected correctly
df_prob1.printSchema()

df_prob1.show(3)

# Aggregating by VictAge and counting the number of crimes
age_counts = df_prob1.groupBy("VictAge").count().orderBy("VictAge")

# Displaying aggregated data
age_counts.show()

# Aggregating by VictSex and counting the number of crimes
sex_counts = df_prob1.groupBy("VictSex").count()

# Displaying aggregated data
sex_counts.show()

# Aggregating by VictDescent and counting the number of crimes
descent_counts = df_prob1.groupBy("VictDescent").count()

# Displaying aggregated data
descent_counts.show()

from pyspark.sql.functions import col, floor

# Creating age bins of 10 years
df_prob1 = df_prob1.withColumn("AgeBin", floor((col("VictAge") - 1) / 10) * 10)

# Aggregating by age bins and counting the number of crimes
age_counts = df_prob1.groupBy("AgeBin").count().orderBy("AgeBin")

# Collecting data to the driver for plotting
age_counts_data = age_counts.collect()

# Extracting labels and values for the plot
age_labels = [f"{bin}-{bin+9}" for bin in range(0, 100, 10)]
age_values = [0] * 10  # Initializing values for each age bin

# Filling in the count values for respective age bins
for row in age_counts_data:
    bin_index = int(row["AgeBin"] / 10)
    age_values[bin_index] = row["count"]

# Plotting VictAge vs Number of Crimes with age bins of 10 years
plt.figure(figsize=(10, 6))
plt.bar(age_labels, age_values)
plt.xlabel('Victim Age Bins')
plt.ylabel('Number of Crimes')
plt.title('Victim Age vs Number of Crimes (Age Bins of 10 years)')
plt.xticks(rotation=45)
plt.show()

# Collecting data to the driver for plotting
sex_counts_data = sex_counts.collect()

# Plotting VictSex vs Number of Crimes
sex_labels = [row["VictSex"] for row in sex_counts_data]
sex_values = [row["count"] for row in sex_counts_data]
plt.figure(figsize=(6, 4))
plt.bar(sex_labels, sex_values)
plt.xlabel('Victim Sex')
plt.ylabel('Number of Crimes')
plt.title('Victim Sex vs Number of Crimes')
plt.show()

descent_counts_data = descent_counts.collect()

# Plotting VictDescent vs Number of Crimes
descent_labels = [row["VictDescent"] for row in descent_counts_data]
descent_values = [row["count"] for row in descent_counts_data]
plt.figure(figsize=(20, 6))
plt.bar(descent_labels, descent_values)
plt.xlabel('Victim Descent')
plt.ylabel('Number of Crimes')
plt.title('Victim Descent vs Number of Crimes')
plt.xticks(rotation=90)
plt.show()

df_prob1.show(3)

from pyspark.sql.functions import col, floor
import matplotlib.pyplot as plt

# Assuming you already have the dataframe 'df_prob1'

# Creating age bins of 10 years
df_prob1 = df_prob1.withColumn("AgeBin", floor((col("VictAge") - 1) / 10) * 10)

# Aggregating by age bins and crime types, counting the number of crimes
age_crime_counts = df_prob1.groupBy("AgeBin", "CrimeType").count().orderBy("AgeBin", "CrimeType")

# Collecting data to the driver for plotting
age_crime_counts_data = age_crime_counts.collect()

# Initialize a dictionary to store counts for each crime type in each age bin
crime_type_counts = {crime_type: [0] * 10 for crime_type in df_prob1.select("CrimeType").distinct().rdd.flatMap(lambda x: x).collect()}

# Filling in the count values for respective age bins and crime types
for row in age_crime_counts_data:
    bin_index = int(row["AgeBin"] / 10)
    crime_type_counts[row["CrimeType"]][bin_index] = row["count"]

# Plotting Victim Age vs Number of Crimes with age bins of 10 years (Stacked Bar Chart)
age_labels = [f"{bin}-{bin+9}" for bin in range(0, 100, 10)]
plt.figure(figsize=(10, 6))

# Stacked bar plot for each crime type
bottom = [0] * 10
for crime_type, counts in crime_type_counts.items():
    plt.bar(age_labels, counts, label=crime_type, bottom=bottom)
    bottom = [sum(x) for x in zip(bottom, counts)]

plt.xlabel('Victim Age Bins')
plt.ylabel('Number of Crimes')
plt.title('Victim Age vs Number of Crimes by Crime Type (Age Bins of 10 years)')
plt.xticks(rotation=45)
plt.legend()
plt.show()

import matplotlib.pyplot as plt
import pyspark.sql.functions as F

# Assuming you have a DataFrame df_prob1

# Grouping by VictSex and CrimeType
sex_crime_counts = df_prob1.groupBy("VictSex", "CrimeType").count()

# Pivot the data to get counts for each VictSex and CrimeType combination
pivot_data = sex_crime_counts.groupBy("VictSex").pivot("CrimeType").agg(F.first("count")).fillna(0)

# Reorder columns for better visualization
pivot_data = pivot_data.select("VictSex", "1", "2")

# Collecting data to the driver for plotting
sex_counts_data = pivot_data.collect()

# Extracting labels and values for plotting
sex_labels = [row["VictSex"] for row in sex_counts_data]
crime_type1_values = [row["1"] for row in sex_counts_data]
crime_type2_values = [row["2"] for row in sex_counts_data]

# Plotting stacked bar plot
fig, ax = plt.subplots(figsize=(8, 6))

bar_width = 0.35
index = range(len(sex_labels))

bar1 = plt.bar(index, crime_type1_values, bar_width, label='CrimeType 1')
bar2 = plt.bar(index, crime_type2_values, bar_width, label='CrimeType 2', bottom=crime_type1_values)

plt.xlabel('Victim Sex')
plt.ylabel('Number of Crimes')
plt.title('Stacked Bar Plot of Crimes by Victim Sex and CrimeType')
plt.xticks(index, sex_labels)
plt.legend()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import pyspark.sql.functions as F

# Assuming you have a DataFrame df_prob1

# Grouping by VictSex and CrimeType
sex_crime_counts = df_prob1.groupBy("VictSex", "CrimeType").count()

# Pivot the data to get counts for each VictSex and CrimeType combination
pivot_data = sex_crime_counts.groupBy("VictSex").pivot("CrimeType").agg(F.first("count")).fillna(0)

# Reorder columns for better visualization
pivot_data = pivot_data.select("VictSex", "1", "2")

# Collecting data to the driver for plotting
sex_counts_data = pivot_data.collect()

# Extracting labels and values for plotting
sex_labels = [row["VictSex"] for row in sex_counts_data]
crime_type1_values = [row["1"] for row in sex_counts_data]
crime_type2_values = [row["2"] for row in sex_counts_data]

# Plotting stacked bar plot
fig, ax = plt.subplots(figsize=(8, 6))

bar_width = 0.35
index = range(len(sex_labels))

bar1 = plt.bar(index, crime_type1_values, bar_width, label='CrimeType 1')
bar2 = plt.bar(index, crime_type2_values, bar_width, label='CrimeType 2', bottom=crime_type1_values)

plt.xlabel('Victim Sex')
plt.ylabel('Number of Crimes')
plt.title('Stacked Bar Plot of Crimes by Victim Sex and CrimeType')
plt.xticks(index, sex_labels)
plt.legend()

# Annotate the counts on the bars
for i, val in enumerate(crime_type1_values):
    plt.text(i, val / 2, str(val), color='white', ha='center', va='center')

for i, val in enumerate(crime_type2_values):
    plt.text(i, val + crime_type1_values[i] / 2, str(val), color='black', ha='center', va='center')

plt.tight_layout()
plt.show()

"""# BUSINESS PROBLEM 2:

What areas have the highest and lowest crime rates? What are the most and least prevelant crimes overall and for an area?
"""

# 1. Areas with the highest and lowest crime rates
crime_rates = df_spark.groupBy("AreaName").count().withColumnRenamed("count", "CrimeCount")
crime_rates = crime_rates.sort(col("CrimeCount").desc())

df_spark.printSchema()
df_spark.show(5)

crime_rates = df_spark.groupBy("AreaName").count().withColumnRenamed("count", "CrimeCount")
crime_rates = crime_rates.sort(col("CrimeCount").desc())

print("Areas with the highest crime rates:")
crime_rates.show(5)

crime_rates.select("AreaName", "CrimeCount").orderBy("CrimeCount").show(5, truncate=False)

# 2. Most and least prevalent crimes overall
overall_crime_types = df_spark.groupBy("CrmCdDesc").count().withColumnRenamed("count", "CrimeCount")
overall_crime_types = overall_crime_types.sort(col("CrimeCount").desc())

# Display most prevalent crimes overall
print("Most prevalent crimes overall:")
overall_crime_types.show(5)

# Display least prevalent crimes overall without truncating column values
print("Least prevalent crimes overall:")
overall_crime_types.select("CrmCdDesc", "CrimeCount").orderBy("CrimeCount").show(5, truncate=False)

area_crime_types = df_spark.groupBy("AreaName", "CrmCdDesc").count().withColumnRenamed("count", "CrimeCount")
area_crime_types = area_crime_types.sort(col("AreaName"), col("CrimeCount").desc())

# Display most prevalent crimes for each area
print("Most prevalent crimes for each area:")
area_crime_types.show(5)

# Display least prevalent crimes for each area without truncating column values
area_crime_types.select("AreaName", "CrmCdDesc", "CrimeCount").orderBy("CrimeCount").show(5, truncate=False)

# Display areas with the lowest crime rates without truncating column values
print("Areas with the lowest crime rates:")
crime_rates.select("AreaName", "CrimeCount").orderBy("CrimeCount").show(5, truncate=False)

# 2. Most and least prevalent crimes overall
overall_crime_types = df_spark.groupBy("CrmCdDesc").count().withColumnRenamed("count", "CrimeCount")
overall_crime_types = overall_crime_types.sort(col("CrimeCount").desc())

# Display most prevalent crimes overall without truncating column values
print("Most prevalent crimes overall:")
overall_crime_types.select("CrmCdDesc", "CrimeCount").show(5, truncate=False)

# Display least prevalent crimes overall without truncating column values
print("Least prevalent crimes overall:")
overall_crime_types.select("CrmCdDesc", "CrimeCount").orderBy("CrimeCount").show(5, truncate=False)

# 3. Most and least prevalent crimes for each area
area_crime_types = df_spark.groupBy("AreaName", "CrmCdDesc").count().withColumnRenamed("count", "CrimeCount")
area_crime_types = area_crime_types.sort(col("AreaName"), col("CrimeCount").desc())

# Display most prevalent crimes for each area without truncating column values
print("Most prevalent crimes for each area")
area_crime_types.select("AreaName", "CrmCdDesc", "CrimeCount").show(15, truncate=False)

# Display least prevalent crimes for each area without truncating column values
print("Least prevalent crimes for each area")
area_crime_types.select("AreaName", "CrmCdDesc", "CrimeCount").orderBy("CrimeCount").show(5, truncate=False)

"""#BUSINESS PROBLEM 3:

Is there a relation between the difference in the time of occurrence and report of a crime depending on the type of crime?
"""

from pyspark.sql import functions as F
import plotly.express as px

# Calculate the time difference in hours
df_spark = df_spark.withColumn("TimeDiff_hours", F.abs((F.col("DateRptd").cast("timestamp").cast("long") - F.col("DateOcc").cast("timestamp").cast("long")) / 3600))

# Calculate the time difference in days
df_spark = df_spark.withColumn("TimeDiff_days", F.abs(F.datediff("DateRptd", "DateOcc")))

# Select the desired columns
result_df = df_spark.select("CrmCdDesc", "DateRptd", "DateOcc", "TimeDiff_hours", "TimeDiff_days")

# Show the resulting DataFrame
result_df.show(truncate=False)

df_spark = df_spark.withColumn('TimeDifference', F.datediff(df_spark['DateRptd'], df_spark['DateOcc']))

grouped_data = df_spark.groupBy('CrmCdDesc')
mean_time_diff = grouped_data.agg(F.mean('TimeDifference').alias('MeanTimeDifference'))

# Sort the DataFrame by mean time difference in descending order
mean_time_diff = mean_time_diff.orderBy('MeanTimeDifference', ascending=False)

# Take the top 20 rows
top_20_mean_time_diff = mean_time_diff.limit(20)

# Convert to Pandas for Plotly
top_20_pd = top_20_mean_time_diff.toPandas()

# Create a bar plot using Plotly
fig = px.bar(top_20_pd, x='CrmCdDesc', y='MeanTimeDifference',
             labels={'MeanTimeDifference': 'Mean Time Difference (days)'},
             title='Top 20 Crime Types by Mean Time Difference',
             orientation='v')

# Show the plot
fig.show()

df_spark = df_spark.withColumn('TimeDifference', F.datediff(df_spark['DateRptd'], df_spark['DateOcc']))

grouped_data = df_spark.groupBy('CrmCdDesc')
mean_time_diff = grouped_data.agg(F.mean('TimeDifference').alias('MeanTimeDifference'))

# Sort the DataFrame by mean time difference in ascending order
mean_time_diff = mean_time_diff.orderBy('MeanTimeDifference', ascending=True)

# Take the bottom 20 rows
bottom_20_mean_time_diff = mean_time_diff.limit(20)

# Convert to Pandas for Plotly
bottom_20_pd = bottom_20_mean_time_diff.toPandas()

# Create a bar plot using Plotly
fig = px.bar(bottom_20_pd, x='CrmCdDesc', y='MeanTimeDifference',
             labels={'MeanTimeDifference': 'Mean Time Difference (days)'},
             title='Bottom 20 Crime Types by Mean Time Difference',
             orientation='v')

# Show the plot
fig.show()

import plotly.subplots as sp
import plotly.graph_objects as go

# Calculate the time difference in hours
df_spark = df_spark.withColumn('TimeDifferenceHours', F.abs((F.col('DateRptd').cast('timestamp').cast('long') - F.col('DateOcc').cast('timestamp').cast('long')) / 3600))

# Group by CrmCdDesc and calculate mean time differences
grouped_data = df_spark.groupBy('CrmCdDesc')
mean_time_diff = grouped_data.agg(F.mean('TimeDifferenceHours').alias('MeanTimeDifferenceHours'))

# Sort the DataFrame by mean time difference in descending order
mean_time_diff_top = mean_time_diff.orderBy('MeanTimeDifferenceHours', ascending=False).limit(10)

# Sort the DataFrame by mean time difference in ascending order
mean_time_diff_bottom = mean_time_diff.orderBy('MeanTimeDifferenceHours', ascending=True).limit(10)

# Convert to Pandas for Plotly
top_10_pd = mean_time_diff_top.toPandas()
bottom_10_pd = mean_time_diff_bottom.toPandas()

# Create subplots
fig = sp.make_subplots(rows=2, cols=1, subplot_titles=['Top 10 Crime Types', 'Bottom 10 Crime Types'])

# Add top 10 bar plot
top_10_bar = px.bar(top_10_pd, x='CrmCdDesc', y='MeanTimeDifferenceHours',
                    labels={'MeanTimeDifferenceHours': 'Mean Time Difference (hours)'},
                    title='Top 10 Crime Types by Mean Time Difference (hours)',
                    orientation='v')
fig.add_trace(top_10_bar['data'][0], row=1, col=1)

# Add bottom 10 bar plot
bottom_10_bar = px.bar(bottom_10_pd, x='CrmCdDesc', y='MeanTimeDifferenceHours',
                       labels={'MeanTimeDifferenceHours': 'Mean Time Difference (hours)'},
                       title='Bottom 10 Crime Types by Mean Time Difference (hours)',
                       orientation='v')
fig.add_trace(bottom_10_bar['data'][0], row=2, col=1)

# Update layout
fig.update_layout(height=800, showlegend=False)

# Set y-axis ticks in gaps of 10 for both subplots
fig.update_yaxes(tickmode='array', row=1, col=1)
fig.update_yaxes(tickmode='array', row=2, col=1)
fig.update_xaxes(tickfont=dict(size=8), tickangle=20)

# Show the plot
fig.show()

# Get counts of unique values in 'VictSex' column
counts_per_vict_descent = df_spark.groupBy('VictDescent').count().orderBy('count', ascending=False)

# Show unique values with counts
counts_per_vict_descent.show()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Replace 'VictAge', 'VictSex', 'VictDescent' with your actual column names
filtered_df = df_spark.filter(
    (col("VictAge") > 0) &
    (col("VictSex") != "NULL") &
    (col("VictDescent").isin(["None", "NULL"]) == False)
)

# Show the resulting DataFrame
filtered_df.show(3)

filtered_df_count = filtered_df.count(); filtered_df_count

# Get the converse of the conditions
converse_df = df_spark.filter(
    ~((col("VictAge") > 0) &
      (col("VictSex") != "NULL") &
      (col("VictDescent").isin(["None", "NULL"]) == False))
)

# Show the converse DataFrame
converse_df.show(3)

converse_df_count = converse_df.count(); converse_df_count

filtered_df_count+converse_df_count

df_spark=filtered_df

df_spark.count()

"""#Feature engineering

New columns made for time_diff in hours and days
"""

# Calculate the time difference in hours and days
df_spark = df_spark.withColumn("Rptd-Occ_TimeDiffHours", F.abs((F.col("DateRptd").cast("timestamp").cast("long") - F.col("DateOcc").cast("timestamp").cast("long")) / 3600))

# Show the resulting DataFrame
df_spark.show(3)

"""New column made for dividing TimeOcc into 4 parts"""

from pyspark.sql.functions import when, col

# Replace 'TimeOcc' values based on conditions
df_spark = df_spark.withColumn(
    'TimeOcc',
    when((col('TimeOcc').between(500, 1159)), 'Morning')
    .when((col('TimeOcc').between(1200, 1659)), 'Afternoon')
    .when((col('TimeOcc').between(1700, 2059)), 'Evening')
    .when((col('TimeOcc').between(2100, 2400) | col('TimeOcc').between(0, 459)), 'Night')
    .otherwise(df_spark['TimeOcc'])
)

# Show the updated DataFrame
df_spark.show()

df_pandas = df_spark.toPandas()

df_pandas.head(5)

# Columns for one-hot encoding
categorical_columns = ['TimeOcc', 'AreaName', 'CrimeType', 'CrmCdDesc']

# Perform one-hot encoding using get_dummies
df_pandas_encoded = pd.get_dummies(df_pandas, columns=categorical_columns)

# List of columns to drop
columns_to_drop = ['DateRptd', 'DateOcc', 'AreaCd', 'CrmCd', 'Mocodes', 'PremisCd', 'Address', 'StatusDesc', 'RptDistNo', 'PremisDesc', 'Weapon', 'Status']

# Drop specified columns
df_pandas_encoded = df_pandas_encoded.drop(columns=columns_to_drop)

df_pandas_encoded.head(5)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

"""#Descent"""

df_pandas_encoded.head(5)

# Select all features except for the target column
X = df_pandas_encoded.drop(['VictDescent','VictSex'], axis=1)
y = df_pandas_encoded['VictDescent']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply PCA to reduce dimensionality to 20 components
pca = PCA(n_components=20)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Create and train the RandomForestClassifier on the reduced feature set
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train_pca, y_train)

# Make predictions on the test set
y_pred = rf_classifier.predict(X_test_pca)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
classification_report_str = classification_report(y_test, y_pred)

print("Accuracy:", accuracy)
print("\nClassification Report:\n", classification_report_str)

"""#Gender"""

# Select all features except for the target column
X = df_pandas_encoded.drop(['VictDescent', 'VictAge', 'VictSex'], axis=1)
y = df_pandas_encoded['VictSex']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.head()

# Apply PCA to reduce dimensionality to 20 components
pca = PCA(n_components=20)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Create and train the RandomForestClassifier on the reduced feature set
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train_pca, y_train)

# Make predictions on the test set
y_pred = rf_classifier.predict(X_test_pca)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
classification_report_str = classification_report(y_test, y_pred)

print("Accuracy:", accuracy)
print("\nClassification Report:\n", classification_report_str)

"""#Age"""

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

from sklearn.decomposition import PCA

# Select all features except for the target column
X = df_pandas_encoded.drop(['VictDescent', 'VictAge', 'VictSex'], axis=1)
y = df_pandas_encoded['VictAge']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply PCA to reduce dimensionality to 20 components
pca = PCA(n_components=20)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Create and train a Linear Regression model
linear_reg = LinearRegression()
linear_reg.fit(X_train_pca, y_train)
y_pred_lr = linear_reg.predict(X_test_pca)

# Evaluate Linear Regression model using Mean Squared Error
mse_lr = mean_squared_error(y_test, y_pred_lr)
print("Linear Regression with PCA MSE:", mse_lr)

from sklearn.tree import DecisionTreeRegressor

# Create and train a DecisionTreeRegressor model
dt_reg = DecisionTreeRegressor(random_state=42)
dt_reg.fit(X_train_pca, y_train)
y_pred_dt = dt_reg.predict(X_test_pca)

# Evaluate Decision Tree model using Mean Squared Error
mse_dt = mean_squared_error(y_test, y_pred_dt)
print("Decision Tree Regression with PCA MSE:", mse_dt)

# Create a RandomForestRegressor model
rf_reg = RandomForestRegressor(random_state=42)

# Fit the model on the training data
rf_reg.fit(X_train_pca, y_train)

y_pred_rfreg = rf_reg.predict(X_test_pca)

mse_rfreg = mean_squared_error(y_test, y_pred_rfreg)
print("Decision Tree Regression with PCA MSE:", mse_rfreg)

from sklearn.metrics import r2_score

# Calculate R-squared for Random Forest
r2_rf = r2_score(y_test, y_pred_rfreg)
print("Random Forest Regression R-squared:", r2_rf)

# Calculate R-squared for Decision Tree
r2_dt = r2_score(y_test, y_pred_dt)
print("Decision Tree Regression R-squared:", r2_dt)

# Calculate R-squared for Linear Regression
r2_lr = r2_score(y_test, y_pred_lr)
print("Linear Regression R-squared:", r2_lr)

"""#Tensorflow"""

pip install tensorflow pandas

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Build the neural network model
model = Sequential()
model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='linear'))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Evaluate the model on the test set
loss = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Build the neural network model
model = Sequential()
model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='linear'))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(X_train, y_train, epochs=15, batch_size=64, validation_split=0.2)

# Evaluate the model on the test set
loss = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss}")

spark.stop()
